<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Team name</title><link>https://ckids-datafest.github.io/2023-fall-forensic/</link><atom:link href="https://ckids-datafest.github.io/2023-fall-forensic/index.xml" rel="self" type="application/rss+xml"/><description>Team name</description><generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Mon, 24 Oct 2022 00:00:00 +0000</lastBuildDate><image><url>https://ckids-datafest.github.io/2023-fall-forensic/media/icon_hu5486d42984c30aaff6be99d37062b147_3155_512x512_fill_lanczos_center_3.png</url><title>Team name</title><link>https://ckids-datafest.github.io/2023-fall-forensic/</link></image><item><title>People</title><link>https://ckids-datafest.github.io/2023-fall-forensic/people/</link><pubDate>Mon, 24 Oct 2022 00:00:00 +0000</pubDate><guid>https://ckids-datafest.github.io/2023-fall-forensic/people/</guid><description/></item><item><title>Approach</title><link>https://ckids-datafest.github.io/2023-fall-forensic/approach/</link><pubDate>Thu, 28 Jun 2018 00:00:00 +0000</pubDate><guid>https://ckids-datafest.github.io/2023-fall-forensic/approach/</guid><description>&lt;p>This page contains key sections of the &lt;strong>Final Report&lt;/strong> for the project focused on the data science methodology used to approach the problem. It should be no more than 3 pages long. It should be done after or in combination with the Requirements document. It should have an initial release after no more than eight weeks into the project, and can serve as an interim project report. It can be refined as the project progresses and the problem is better understood.&lt;/p>
&lt;h2 id="data-quality">Data Quality&lt;/h2>
&lt;p>Describe any steps that were used to address any issues concerning the quality of the data. This may include collecting data quality metrics, discarding subsets of the data, or applying specific techniques for handling missing values, dealing with outliers, etc.&lt;/p>
&lt;h2 id="data-preprocessing">Data Preprocessing&lt;/h2>
&lt;p>Describe the steps taken to preprocess the raw data to prepare it for analysis. This may include data transformations to convert to a required format, feature engineering operations, encoding features as binary, etc.&lt;/p>
&lt;h2 id="exploratory-data-analysis-eda">Exploratory Data Analysis (EDA)&lt;/h2>
&lt;p>Discuss any techniques employed to gain insights into the data. This could include data visualizations, generating summary statistics, initial analysis, and other exploratory techniques used to understand the data distributions, features, and helpful patterns.&lt;/p>
&lt;h2 id="model-development">Model Development&lt;/h2>
&lt;p>Describe the algorithms, methodology, and architectures used to generate models. Discuss how models were generated, seeded, and improved. Show the libraries and frameworks used for model development, as well as the rationale behind those choices.&lt;/p>
&lt;h2 id="model-evaluation">Model Evaluation&lt;/h2>
&lt;p>Discuss the evaluation metrics used to assess model performance, and justify those choices based on the problem that the project is addressing. Describe the evaluation techniques used, such as cross-validation, and how undesirable model behaviors, such as overfitting, were avoided.&lt;/p></description></item><item><title>Data Assessment</title><link>https://ckids-datafest.github.io/2023-fall-forensic/data/</link><pubDate>Thu, 28 Jun 2018 00:00:00 +0000</pubDate><guid>https://ckids-datafest.github.io/2023-fall-forensic/data/</guid><description>&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>The Data Assessment Document introduces a comprehensive overview of our project, emphasizing its role as an interim report in the initial project stages. This document serves to evaluate and refine our understanding of the data utilized.&lt;/p>
&lt;h2 id="data-overview-and-examples">Data Overview and Examples&lt;/h2>
&lt;p>This is an overview of a Automated Question Type Coding dataset:
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://github.com/ckids-datafirst/2023-fall-forensic/blob/main/assets/media/question%20types.png?raw=true" alt="Accuracy" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;h2 id="data-accessibility">Data Accessibility&lt;/h2>
&lt;p>Our groundbreaking dataset, constituting the first large language model trained on authentic forensic interviews and court transcripts, encompasses 349,033 utterances drawn from 1,851 transcripts. This diverse compilation includes forensic interviews conducted in California from 2004 to 2022 (1,435 transcripts, ages 3-17, M = 7.81), court trials in Los Angeles County Court from 1997 to 2001 (416 transcripts, ages 4-17, M = 11.85), manually question type coded by coders trained to achieve high reliability.&lt;/p>
&lt;h2 id="data-formats">Data Formats&lt;/h2>
&lt;p>Data is presented in comma seperated values (CSV) format.&lt;/p>
&lt;h2 id="data-challenges">Data Challenges&lt;/h2>
&lt;p>A notable challenge in our dataset lies in the low frequency of invitations, posing a potential hurdle for training models effectively. This scarcity demands strategic approaches to ensure the robustness and accuracy of automated question type coding systems in capturing the nuances of less common question types.&lt;/p></description></item><item><title>Problem and Requirements</title><link>https://ckids-datafest.github.io/2023-fall-forensic/problem-statement/</link><pubDate>Thu, 28 Jun 2018 00:00:00 +0000</pubDate><guid>https://ckids-datafest.github.io/2023-fall-forensic/problem-statement/</guid><description>&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>In the realm of forensic interviewing research, the meticulous process of question type coding plays a pivotal role in distinguishing best practice open-ended questions from the undesirable closed-ended and leading questions that interviewers are trained to avoid. Traditionally, research teams have grappled with a time-consuming and labor-intensive approach to question type coding, wherein each question in an interview is manually coded by a researcher, with a subset subjected to a second researcher&amp;rsquo;s coding to demonstrate inter-rater reliability.&lt;/p>
&lt;p>In response to this challenge, our team has embarked on an innovative initiative by harnessing the capabilities of a large language model, specifically RoBERTa, to discern question types based on a rudimentary classification system. This approach not only promises efficiency gains but also marks a departure from the reliance on mock interviews, as our model has been trained on a substantial dataset comprising 349,033 real forensic interview questions.&lt;/p>
&lt;p>As we advance into the next phase of our project, our focus shifts towards fine-tuning the model and employing zero-shot and few-shot prompting techniques to make distinctions where there is limited manually-coded data. This research stands at the forefront of leveraging automated coding to provide swift and straightforward measures of testimonial quality, contributing to a nuanced understanding of interview dynamics in the field.&lt;/p>
&lt;h2 id="motivation">Motivation&lt;/h2>
&lt;p>Question type coding is integral to evaluating interview quality, a concept extensively discussed by Lamb et al. in their 2018 study. The preferred measure for assessing interview quality emphasizes the significance of employing broad, open-ended questions like &amp;ldquo;Tell me everything that happened,&amp;rdquo; as they effectively elicit comprehensive details while minimizing the risk of misinformation. Wh-questions, such as &amp;ldquo;Where was your mom?&amp;rdquo; are considered more favorable compared to option-posing questions like &amp;ldquo;Did you run away?&amp;rdquo; which interviewers are trained to avoid. Despite its crucial role, the current manual approach to question-type coding is both time-consuming and labor-intensive.&lt;/p>
&lt;p>The introduction of automated coding represents a significant advancement, offering forensic interviewers and attorneys a swift means of obtaining measures of interview quality. This transition is particularly noteworthy given the traditional manual nature of the process. Earlier research in this field relied on mock transcripts, utilizing adults as child witnesses to train models. However, the human-machine reliability of these automated systems has yielded mixed results, indicating the ongoing need for refinement and further exploration to enhance the efficiency and accuracy of automated question-type coding systems.&lt;/p>
&lt;h2 id="problem-for-the-semester">Problem for the Semester&lt;/h2>
&lt;p>Our team&amp;rsquo;s initial model successfully discriminated among three question types: non-questions (e.g., &amp;ldquo;Please sit down.&amp;rdquo;), option-posing questions involving yes/no and forced choice, and invitations &amp;amp; directives, which encompass broad narrative prompts, wh- and how questions. The current project focuses on advancing the model to include a fourth type, specifically targeting the distinction between invitations and directives. The preference for invitations stems from their broader nature, facilitating the extraction of more extensive narrative information. However, a significant challenge lies in the comparatively infrequent use of invitations, especially in legal contexts, resulting in an uneven distribution of question types.&lt;/p>
&lt;h2 id="design-and-approach">Design and Approach&lt;/h2>
&lt;p>To address the issue of data imbalance, we implemented a proactive strategy involving the oversampling of invitation examples to prevent potential bias within our dataset. This deliberate approach aimed to ensure the model receives ample exposure to instances of invitations, compensating for their relatively lower frequency. Furthermore, we prioritized the robust training of our model by incorporating a diverse dataset, covering a wide array of scenarios and contexts to enhance its overall adaptability. When it came to model selection and fine-tuning, we chose to employ RoBERTa from Hugging Face as the pre-trained model, capitalizing on its advanced features. The subsequent fine-tuning process focused on optimizing the model&amp;rsquo;s performance using the oversampled dataset, aligning it more closely with the subtleties of invitation-based questions.&lt;/p>
&lt;h2 id="desired-outcomes-and-benefits">Desired Outcomes and Benefits&lt;/h2>
&lt;p>The implementation of automated coding in the field of forensic interviews holds the promise of significantly enhancing efficiency and expediting processes for interviewers, attorneys, and expert witnesses. By automating the coding of question types, valuable time and resources can be saved, allowing these professionals to focus more on the substantive aspects of their work.&lt;/p>
&lt;p>The envisioned outcomes include streamlined workflows during interviews and legal proceedings, reducing the manual labor associated with question type coding. Moreover, the incorporation of transcription services into the automated system adds an additional layer of convenience, enabling quick and accurate documentation of interviews. Furthermore, the development of an instant feedback system promises to provide timely insights into interview quality, empowering practitioners to make real-time adjustments and improvements.&lt;/p>
&lt;p>Overall, the integration of automated coding, transcription, and instant feedback systems represents a transformative step towards enhancing the effectiveness and efficiency of forensic interview processes.&lt;/p></description></item><item><title>Results</title><link>https://ckids-datafest.github.io/2023-fall-forensic/results/</link><pubDate>Thu, 28 Jun 2018 00:00:00 +0000</pubDate><guid>https://ckids-datafest.github.io/2023-fall-forensic/results/</guid><description>&lt;h2 id="system-and-model-performance">System and Model Performance&lt;/h2>
&lt;ul>
&lt;li>Previous model&amp;rsquo;s performance: High human-machine agreement (Cohen’s Kappa: .89); 88% for non-questions, 94% for option-posing questions, and 95% for wh-questions/invitations&lt;/li>
&lt;li>New model’s performance: High human-machine agreement
&lt;ul>
&lt;li>82% for non-questions&lt;/li>
&lt;li>82% for option-posing questions,&lt;/li>
&lt;li>98% for wh-questions&lt;/li>
&lt;li>96% for invitations&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://raw.githubusercontent.com/ckids-datafirst/2023-fall-forensic/main/assets/media/accuracy.png" alt="Accuracy" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://github.com/ckids-datafirst/2023-fall-forensic/blob/main/assets/media/confusion%20matrix.png?raw=true" alt="Confusion Matrix" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;h2 id="future-work">Future Work&lt;/h2>
&lt;ul>
&lt;li>Differentiate between subtypes of option-posing questions
&lt;ul>
&lt;li>Yes-no questions (“Did you go home that night?”)&lt;/li>
&lt;li>Forced choice questions (“Did you go home that night or did you stay with your sister?”)&lt;/li>
&lt;li>Suggestive questions (“You went home that night, didn’t you?”)&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Categorize children’s responses
&lt;ul>
&lt;li>Unelaborated (Q: “Did you go home that night?” A: “No.”)&lt;/li>
&lt;li>Elaborated (Q: “Did you go home that night?” A: “No, I stayed with my sister.”)&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul></description></item></channel></rss>