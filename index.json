[{"authors":null,"categories":null,"content":"","date":1666569600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1701378912,"objectID":"c1d17ff2b20dca0ad6653a3161942b64","permalink":"https://ckids-datafest.github.io/2023-fall-forensic/people/","publishdate":"2022-10-24T00:00:00Z","relpermalink":"/2023-fall-forensic/people/","section":"","summary":"","tags":null,"title":"People","type":"landing"},{"authors":null,"categories":null,"content":"This page contains key sections of the Final Report for the project focused on the data science methodology used to approach the problem. It should be no more than 3 pages long. It should be done after or in combination with the Requirements document. It should have an initial release after no more than eight weeks into the project, and can serve as an interim project report. It can be refined as the project progresses and the problem is better understood.\nData Quality Describe any steps that were used to address any issues concerning the quality of the data. This may include collecting data quality metrics, discarding subsets of the data, or applying specific techniques for handling missing values, dealing with outliers, etc.\nData Preprocessing Describe the steps taken to preprocess the raw data to prepare it for analysis. This may include data transformations to convert to a required format, feature engineering operations, encoding features as binary, etc.\nExploratory Data Analysis (EDA) Discuss any techniques employed to gain insights into the data. This could include data visualizations, generating summary statistics, initial analysis, and other exploratory techniques used to understand the data distributions, features, and helpful patterns.\nModel Development Describe the algorithms, methodology, and architectures used to generate models. Discuss how models were generated, seeded, and improved. Show the libraries and frameworks used for model development, as well as the rationale behind those choices.\nModel Evaluation Discuss the evaluation metrics used to assess model performance, and justify those choices based on the problem that the project is addressing. Describe the evaluation techniques used, such as cross-validation, and how undesirable model behaviors, such as overfitting, were avoided.\n","date":1530144e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1696363185,"objectID":"3e050718825977bdecb55c075afa314a","permalink":"https://ckids-datafest.github.io/2023-fall-forensic/approach/","publishdate":"2018-06-28T00:00:00Z","relpermalink":"/2023-fall-forensic/approach/","section":"","summary":"Data science methodology used to address the problem, including data preprocessing steps, exploratory data analysis, feature engineering techniques, machine learning models, and evaluation metrics.","tags":null,"title":"Approach","type":"page"},{"authors":null,"categories":null,"content":"Introduction This page serves as a Data Assessment Document for the project. It should be no more than 2-3 pages long. It can be drafted in the first two weeks of the project, and can serve as an interim project report. It can be refined as the project progresses and the use of the data is better understood.\nData Overview and Examples Give a brief description of the data provided for this project, what it represents, how it was collected, and why it may help address the problem you are tackling. Discuss if you will be using all the data or only some subset of it for the project. Consider possible additional data that may be publicly available in the open Web that might complement the data that you are given.\nData Accessibility Summarize how the data can be accessed. For example, data may be available for download in files, or accessible through an API, or can be queried from a database. Mention any restrictions in accessing the data, for example if it is sensitive data that can only accessed with special permission.\nData Formats Describe briefly the formats of the data. Common data formats include CSV, JSON, XML, shapefiles, or any other specific formats relevant to your website.\nData Challenges Summarize why analyzing this data will be challenging. This may include issues like data heterogeneity, data size, and any pre- or post-processing needs. Explain some ideas for how these challenges could be addressed.\nData Visualizations and Highlights Including a visualization is a simple way to show something interesting about the data. Perhaps the visualizations could simply highlight the size, distribution, and other simple statistical characteristics of the data.\n","date":1530144e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1696363185,"objectID":"be566fdb6f0fa08cfea50d77a89a6b5a","permalink":"https://ckids-datafest.github.io/2023-fall-forensic/data/","publishdate":"2018-06-28T00:00:00Z","relpermalink":"/2023-fall-forensic/data/","section":"","summary":"Data Assessment Document that gives an overview of the data used for the project.","tags":null,"title":"Data Assessment","type":"page"},{"authors":null,"categories":null,"content":"Introduction In the realm of forensic interviewing research, the meticulous process of question type coding plays a pivotal role in distinguishing best practice open-ended questions from the undesirable closed-ended and leading questions that interviewers are trained to avoid. Traditionally, research teams have grappled with a time-consuming and labor-intensive approach to question type coding, wherein each question in an interview is manually coded by a researcher, with a subset subjected to a second researcher’s coding to demonstrate inter-rater reliability. In response to this challenge, our team has embarked on an innovative initiative by harnessing the capabilities of a large language model, specifically RoBERTa, to discern question types based on a rudimentary classification system. This approach not only promises efficiency gains but also marks a departure from the reliance on mock interviews, as our model has been trained on a substantial dataset comprising 349,033 real forensic interview questions. As we advance into the next phase of our project, our focus shifts towards fine-tuning the model and employing zero-shot and few-shot prompting techniques to make distinctions where there is limited manually-coded data. This research stands at the forefront of leveraging automated coding to provide swift and straightforward measures of testimonial quality, contributing to a nuanced understanding of interview dynamics in the field.\nMotivation Question type coding is integral to evaluating interview quality, a concept extensively discussed by Lamb et al. in their 2018 study. The preferred measure for assessing interview quality emphasizes the significance of employing broad, open-ended questions like “Tell me everything that happened,” as they effectively elicit comprehensive details while minimizing the risk of misinformation. Wh-questions, such as “Where was your mom?” are considered more favorable compared to option-posing questions like “Did you run away?” which interviewers are trained to avoid. Despite its crucial role, the current manual approach to question-type coding is both time-consuming and labor-intensive. The introduction of automated coding represents a significant advancement, offering forensic interviewers and attorneys a swift means of obtaining measures of interview quality. This transition is particularly noteworthy given the traditional manual nature of the process. Earlier research in this field relied on mock transcripts, utilizing adults as child witnesses to train models. However, the human-machine reliability of these automated systems has yielded mixed results, indicating the ongoing need for refinement and further exploration to enhance the efficiency and accuracy of automated question-type coding systems.\nProblem for the Semester Our team’s initial model successfully discriminated among three question types: non-questions (e.g., “Please sit down.”), option-posing questions involving yes/no and forced choice, and invitations \u0026amp; directives, which encompass broad narrative prompts, wh- and how questions. The current project focuses on advancing the model to include a fourth type, specifically targeting the distinction between invitations and directives. The preference for invitations stems from their broader nature, facilitating the extraction of more extensive narrative information. However, a significant challenge lies in the comparatively infrequent use of invitations, especially in legal contexts, resulting in an uneven distribution of question types.\nDesign and Approach To address the issue of data imbalance, we implemented a proactive strategy involving the oversampling of invitation examples to prevent potential bias within our dataset. This deliberate approach aimed to ensure the model receives ample exposure to instances of invitations, compensating for their relatively lower frequency. Furthermore, we prioritized the robust training of our model by incorporating a diverse dataset, covering a wide array of scenarios and contexts to enhance its overall adaptability. When it came to model selection and fine-tuning, we chose to employ RoBERTa from Hugging Face as the pre-trained model, capitalizing on its advanced features. The subsequent fine-tuning process focused on optimizing the model’s performance using the oversampled dataset, aligning it more closely with the subtleties of invitation-based questions.\nDesired Outcomes and Benefits The implementation of automated coding in the field of forensic interviews holds the promise of significantly enhancing efficiency and expediting processes for interviewers, attorneys, and expert witnesses. By automating the coding of question types, valuable time and resources can be saved, allowing these professionals to focus more on the substantive aspects of their work. The envisioned outcomes include streamlined workflows during interviews and legal proceedings, reducing the manual labor associated with question type coding. Moreover, …","date":1530144e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1701382301,"objectID":"b7c3446bb0d5d7e8a477294017361379","permalink":"https://ckids-datafest.github.io/2023-fall-forensic/problem-statement/","publishdate":"2018-06-28T00:00:00Z","relpermalink":"/2023-fall-forensic/problem-statement/","section":"","summary":"Problem and Requirements document that will drive the work to be done in the project","tags":null,"title":"Problem and Requirements","type":"page"},{"authors":null,"categories":null,"content":"This page contains key sections of the Final Report for the project focused on results to date. It should be no more than 2 pages long. An initial draft can be created at any point during the project, and can be refined as the project progresses.\nSystem and Model Performance Show the performance of the best system and model(s) developed, showing clearly the performance metrics and improvements over the baseline system as appropriate. Create visualizations that show clearly these results.\nDiscussion of Findings Offer a discussion of the main findings using the system developed. Put the results in the context of the original problem statement and the questions that were posed.\nDiscuss any unexpected results, and potential explanations.\nEnumerate (ideally in bullets) the most important findings, and their impact on your project goals.\nLimitations and Future Work Discuss any limitations of the work to date, how these limitations could be addressed in future work. Discuss what lines of work are most promising given the understanding of the problem and the data gained throughout the project.\n","date":1530144e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1696363185,"objectID":"c29e41198fe1dc5c85e66dbe4f2d7737","permalink":"https://ckids-datafest.github.io/2023-fall-forensic/results/","publishdate":"2018-06-28T00:00:00Z","relpermalink":"/2023-fall-forensic/results/","section":"","summary":"The main results of the work done to date","tags":null,"title":"Results","type":"page"}]