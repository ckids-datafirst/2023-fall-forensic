[{"authors":null,"categories":null,"content":"","date":1666569600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1701378912,"objectID":"c1d17ff2b20dca0ad6653a3161942b64","permalink":"https://ckids-datafest.github.io/2023-fall-forensic/people/","publishdate":"2022-10-24T00:00:00Z","relpermalink":"/2023-fall-forensic/people/","section":"","summary":"","tags":null,"title":"People","type":"landing"},{"authors":null,"categories":null,"content":"This page contains key sections of the Final Report for the project focused on the data science methodology used to approach the problem. It should be no more than 3 pages long. It should be done after or in combination with the Requirements document. It should have an initial release after no more than eight weeks into the project, and can serve as an interim project report. It can be refined as the project progresses and the problem is better understood.\nData Quality Describe any steps that were used to address any issues concerning the quality of the data. This may include collecting data quality metrics, discarding subsets of the data, or applying specific techniques for handling missing values, dealing with outliers, etc.\nData Preprocessing Describe the steps taken to preprocess the raw data to prepare it for analysis. This may include data transformations to convert to a required format, feature engineering operations, encoding features as binary, etc.\nExploratory Data Analysis (EDA) Discuss any techniques employed to gain insights into the data. This could include data visualizations, generating summary statistics, initial analysis, and other exploratory techniques used to understand the data distributions, features, and helpful patterns.\nModel Development Describe the algorithms, methodology, and architectures used to generate models. Discuss how models were generated, seeded, and improved. Show the libraries and frameworks used for model development, as well as the rationale behind those choices.\nModel Evaluation Discuss the evaluation metrics used to assess model performance, and justify those choices based on the problem that the project is addressing. Describe the evaluation techniques used, such as cross-validation, and how undesirable model behaviors, such as overfitting, were avoided.\n","date":1530144e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1696363185,"objectID":"3e050718825977bdecb55c075afa314a","permalink":"https://ckids-datafest.github.io/2023-fall-forensic/approach/","publishdate":"2018-06-28T00:00:00Z","relpermalink":"/2023-fall-forensic/approach/","section":"","summary":"Data science methodology used to address the problem, including data preprocessing steps, exploratory data analysis, feature engineering techniques, machine learning models, and evaluation metrics.","tags":null,"title":"Approach","type":"page"},{"authors":null,"categories":null,"content":"Introduction The Data Assessment Document introduces a comprehensive overview of our project, emphasizing its role as an interim report in the initial project stages. This document serves to evaluate and refine our understanding of the data utilized.\nData Overview and Examples This is an overview of a Automated Question Type Coding dataset: Data Accessibility Our groundbreaking dataset, constituting the first large language model trained on authentic forensic interviews and court transcripts, encompasses 349,033 utterances drawn from 1,851 transcripts. This diverse compilation includes forensic interviews conducted in California from 2004 to 2022 (1,435 transcripts, ages 3-17, M = 7.81), court trials in Los Angeles County Court from 1997 to 2001 (416 transcripts, ages 4-17, M = 11.85), manually question type coded by coders trained to achieve high reliability.\nData Formats Data is presented in comma seperated values (CSV) format.\nData Challenges A notable challenge in our dataset lies in the low frequency of invitations, posing a potential hurdle for training models effectively. This scarcity demands strategic approaches to ensure the robustness and accuracy of automated question type coding systems in capturing the nuances of less common question types.\n","date":1530144e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1701390446,"objectID":"be566fdb6f0fa08cfea50d77a89a6b5a","permalink":"https://ckids-datafest.github.io/2023-fall-forensic/data/","publishdate":"2018-06-28T00:00:00Z","relpermalink":"/2023-fall-forensic/data/","section":"","summary":"Data Assessment Document that gives an overview of the data used for the project.","tags":null,"title":"Data Assessment","type":"page"},{"authors":null,"categories":null,"content":"Introduction In the realm of forensic interviewing research, the meticulous process of question type coding plays a pivotal role in distinguishing best practice open-ended questions from the undesirable closed-ended and leading questions that interviewers are trained to avoid. Traditionally, research teams have grappled with a time-consuming and labor-intensive approach to question type coding, wherein each question in an interview is manually coded by a researcher, with a subset subjected to a second researcher’s coding to demonstrate inter-rater reliability.\nIn response to this challenge, our team has embarked on an innovative initiative by harnessing the capabilities of a large language model, specifically RoBERTa, to discern question types based on a rudimentary classification system. This approach not only promises efficiency gains but also marks a departure from the reliance on mock interviews, as our model has been trained on a substantial dataset comprising 349,033 real forensic interview questions.\nAs we advance into the next phase of our project, our focus shifts towards fine-tuning the model and employing zero-shot and few-shot prompting techniques to make distinctions where there is limited manually-coded data. This research stands at the forefront of leveraging automated coding to provide swift and straightforward measures of testimonial quality, contributing to a nuanced understanding of interview dynamics in the field.\nMotivation Question type coding is integral to evaluating interview quality, a concept extensively discussed by Lamb et al. in their 2018 study. The preferred measure for assessing interview quality emphasizes the significance of employing broad, open-ended questions like “Tell me everything that happened,” as they effectively elicit comprehensive details while minimizing the risk of misinformation. Wh-questions, such as “Where was your mom?” are considered more favorable compared to option-posing questions like “Did you run away?” which interviewers are trained to avoid. Despite its crucial role, the current manual approach to question-type coding is both time-consuming and labor-intensive.\nThe introduction of automated coding represents a significant advancement, offering forensic interviewers and attorneys a swift means of obtaining measures of interview quality. This transition is particularly noteworthy given the traditional manual nature of the process. Earlier research in this field relied on mock transcripts, utilizing adults as child witnesses to train models. However, the human-machine reliability of these automated systems has yielded mixed results, indicating the ongoing need for refinement and further exploration to enhance the efficiency and accuracy of automated question-type coding systems.\nProblem for the Semester Our team’s initial model successfully discriminated among three question types: non-questions (e.g., “Please sit down.”), option-posing questions involving yes/no and forced choice, and invitations \u0026amp; directives, which encompass broad narrative prompts, wh- and how questions. The current project focuses on advancing the model to include a fourth type, specifically targeting the distinction between invitations and directives. The preference for invitations stems from their broader nature, facilitating the extraction of more extensive narrative information. However, a significant challenge lies in the comparatively infrequent use of invitations, especially in legal contexts, resulting in an uneven distribution of question types.\nDesign and Approach To address the issue of data imbalance, we implemented a proactive strategy involving the oversampling of invitation examples to prevent potential bias within our dataset. This deliberate approach aimed to ensure the model receives ample exposure to instances of invitations, compensating for their relatively lower frequency. Furthermore, we prioritized the robust training of our model by incorporating a diverse dataset, covering a wide array of scenarios and contexts to enhance its overall adaptability. When it came to model selection and fine-tuning, we chose to employ RoBERTa from Hugging Face as the pre-trained model, capitalizing on its advanced features. The subsequent fine-tuning process focused on optimizing the model’s performance using the oversampled dataset, aligning it more closely with the subtleties of invitation-based questions.\nDesired Outcomes and Benefits The implementation of automated coding in the field of forensic interviews holds the promise of significantly enhancing efficiency and expediting processes for interviewers, attorneys, and expert witnesses. By automating the coding of question types, valuable time and resources can be saved, allowing these professionals to focus more on the substantive aspects of their work.\nThe envisioned outcomes include streamlined workflows during interviews and legal proceedings, reducing the manual labor associated with question type coding. Moreover, …","date":1530144e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1701390446,"objectID":"b7c3446bb0d5d7e8a477294017361379","permalink":"https://ckids-datafest.github.io/2023-fall-forensic/problem-statement/","publishdate":"2018-06-28T00:00:00Z","relpermalink":"/2023-fall-forensic/problem-statement/","section":"","summary":"Problem and Requirements document that will drive the work to be done in the project","tags":null,"title":"Problem and Requirements","type":"page"},{"authors":null,"categories":null,"content":"System and Model Performance Previous model’s performance: High human-machine agreement (Cohen’s Kappa: .89); 88% for non-questions, 94% for option-posing questions, and 95% for wh-questions/invitations New model’s performance: High human-machine agreement 82% for non-questions 82% for option-posing questions, 98% for wh-questions 96% for invitations Future Work Differentiate between subtypes of option-posing questions Yes-no questions (“Did you go home that night?”) Forced choice questions (“Did you go home that night or did you stay with your sister?”) Suggestive questions (“You went home that night, didn’t you?”) Categorize children’s responses Unelaborated (Q: “Did you go home that night?” A: “No.”) Elaborated (Q: “Did you go home that night?” A: “No, I stayed with my sister.”) ","date":1530144e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1701384800,"objectID":"c29e41198fe1dc5c85e66dbe4f2d7737","permalink":"https://ckids-datafest.github.io/2023-fall-forensic/results/","publishdate":"2018-06-28T00:00:00Z","relpermalink":"/2023-fall-forensic/results/","section":"","summary":"The main results of the work done to date","tags":null,"title":"Results","type":"page"}]